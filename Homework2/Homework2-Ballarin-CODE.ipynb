{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRboflunHqye"
   },
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T15:54:27.037721Z",
     "start_time": "2020-11-26T15:54:13.477991Z"
    },
    "id": "ODcSwFcDXrtD"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # plotting library\n",
    "import numpy as np # this module is useful to work with numerical arrays\n",
    "import pandas as pd # this module is useful to work with tabular data\n",
    "import random # this module will be used to select random samples from a collection\n",
    "import os # this module will be used just to create directories in the local filesystem\n",
    "from tqdm import tqdm # this module is useful to plot progress bars\n",
    "import plotly.express as px\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# Pytorch lightning wrapper\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Callback\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# Hyperparams optimization\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcRLWzyP-V0Y"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVIIMElvUQui"
   },
   "source": [
    "## Define the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42-iIce_PqqD"
   },
   "source": [
    "For this lab. we will use one of the dataset already included in PyTorch ([https://pytorch.org/docs/stable/torchvision/datasets.html#mnist](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist)). These dataset do not require the definition of a custom `Dataset` class, so we can focus on the network implementation.\n",
    "\n",
    "The MNIST dataset is a colletion of hand-written digits. The size of the images is $28 \\times 28$, and there is a single channel only (black and white images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T15:55:08.926103Z",
     "start_time": "2020-11-26T15:54:56.675692Z"
    },
    "id": "Ix4XuZUHIdan"
   },
   "outputs": [],
   "source": [
    "### Download the data and create dataset\n",
    "data_dir = 'dataset'\n",
    "# With these commands the train and test datasets, respectively, are downloaded \n",
    "# automatically and stored in the local \"data_dir\" directory.\n",
    "train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST(data_dir, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHZDOuceTvR3"
   },
   "source": [
    "Let's plot some random samples from the dataset. The first element of the sample is the actual image, while the second is the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T15:55:18.956363Z",
     "start_time": "2020-11-26T15:55:15.931852Z"
    },
    "id": "xPUDQhqfSCLD"
   },
   "outputs": [],
   "source": [
    "### Plot some sample\n",
    "fig, axs = plt.subplots(5, 5, figsize=(8,8))\n",
    "for ax in axs.flatten():\n",
    "    # random.choice allows to randomly sample from a list-like object (basically anything that can be accessed with an index, like our dataset)\n",
    "    img, label = random.choice(train_dataset)\n",
    "    ax.imshow(np.array(img), cmap='gist_gray')\n",
    "    ax.set_title('Label: %d' % label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO2HuJ7oUTpg"
   },
   "source": [
    "## Define the dataset transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jro9HO1eUYgg"
   },
   "source": [
    "In this example we are using the input images without any modification. As always, the only requirement is to transform the input data to tensors of the proper shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T15:55:29.903090Z",
     "start_time": "2020-11-26T15:55:29.894070Z"
    },
    "id": "leH6UWCzRxgh"
   },
   "outputs": [],
   "source": [
    "# In this case the train_transform and test_transform are the same, but we keep them separate for potential future updates\n",
    "angles = 45\n",
    "gaussian_kernel = 5\n",
    "#rotate = transforms.RandomRotation(angles)\n",
    "#noise = transforms.GaussianBlur(gaussian_kernel)\n",
    "#random_transform = transforms.RandomChoice([rotate, noise])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "#    random_transform, \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTaTAhRNVP1G"
   },
   "source": [
    "Since we already defined our datasets, this is an alternative (and recommended) way to add (or modify) a dataset transformation without reinitializing the dataset (very useful when the dataset initialization is slow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T15:55:30.988407Z",
     "start_time": "2020-11-26T15:55:30.984430Z"
    },
    "id": "BfVPSi-GVO4f"
   },
   "outputs": [],
   "source": [
    "# Set the train transform\n",
    "train_dataset.transform = train_transform\n",
    "# Set the test transform\n",
    "test_dataset.transform = test_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEkgqEKoUWWo"
   },
   "source": [
    "## Define the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vWlV8dpU67U"
   },
   "source": [
    "The dataloader allows to easily create batch of data, in this case we set a batch size of 256, and we also enable data shuffling for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T15:55:32.759565Z",
     "start_time": "2020-11-26T15:55:32.539055Z"
    },
    "id": "Rr5CULQXcSXG"
   },
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(train_dataset, \n",
    "                                                         [50000, 10000])\n",
    "### Define train dataloader\n",
    "train_dataloader = DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "### Define validation dataloader\n",
    "val_dataloader = DataLoader(val_set, batch_size=256, shuffle=False)\n",
    "### Define test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "batch_data, batch_labels = next(iter(train_dataloader))\n",
    "print(f\"TRAIN BATCH SHAPE\")\n",
    "print(f\"\\t Data: {batch_data.shape}\")\n",
    "print(f\"\\t Labels: {batch_labels.shape}\")\n",
    "\n",
    "batch_data, batch_labels = next(iter(test_dataloader))\n",
    "print(f\"TEST BATCH SHAPE\")\n",
    "print(f\"\\t Data: {batch_data.shape}\")\n",
    "print(f\"\\t Labels: {batch_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHyaO01gVwrC"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters optimization\n",
    "\n",
    "We use Optuna and pytorch lightning to optimize the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder_HP(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, trial):\n",
    "        super().__init__()\n",
    "        encoded_space_dim = trial.suggest_int('encoded_space_dim', 2, 20)\n",
    "        self.trial = trial\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "                                        # First convolutional layer\n",
    "                                        nn.Conv2d(1, 8, kernel_size=3, padding=1, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Second convolutional layer\n",
    "                                        nn.Conv2d(8, 16, kernel_size=3, padding=1, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Third convolutional layer\n",
    "                                        nn.Conv2d(16, 32, kernel_size=3, padding=0, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Flatten layer\n",
    "                                        nn.Flatten(start_dim=1),\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(288, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Second linear layer (output layer)\n",
    "                                        nn.Linear(64, encoded_space_dim)\n",
    "                                    )\n",
    "        # Decoder\n",
    "        self.decoder =  nn.Sequential(\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(encoded_space_dim, 64),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Second linear layer\n",
    "                                        nn.Linear(64, 3*3*32), # (64, 288)\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Unflatten\n",
    "                                        nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)),\n",
    "                                        # First transposed convolution\n",
    "                                        nn.ConvTranspose2d(32, 16, kernel_size=3, output_padding=0, stride=2),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Second transposed convolution\n",
    "                                        nn.ConvTranspose2d(16, 8, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Third transposed convolution\n",
    "                                        nn.ConvTranspose2d(8, 1, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                                        # To obtain an output in [0,1]\n",
    "                                        nn.Sigmoid()\n",
    "                                    )\n",
    "        self.configure_loss(loss_fn)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = self.loss_fn(x_hat, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, loss_name='validation_loss'):\n",
    "        x, y = batch\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        val_loss = self.loss_fn(x_hat, x)\n",
    "        self.log(loss_name, val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.validation_step(batch, batch_idx, loss_name='test_loss')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = self.trial.suggest_categorical('optimizer', ['SGD', 'Adam'])\n",
    "        lr = self.trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "        wd = self.trial.suggest_loguniform('weight_decay', 1e-5, 1e-3)\n",
    "        if opt=='Adam':\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=lr, \n",
    "                                        weight_decay=wd)\n",
    "        elif opt=='SGD':\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=lr, \n",
    "                                        momentum=0.9, weight_decay=wd)\n",
    "            \n",
    "        return optimizer\n",
    "    \n",
    "    def configure_loss(self, loss_fn):\n",
    "        self.loss_fn = loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.metrics.append(trainer.callback_metrics)\n",
    "\n",
    "PERCENT_TEST_EXAMPLES = 1\n",
    "def objective(trial):\n",
    "    # Function to optimize from optuna\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        os.path.join(MODEL_DIR, \"trial_{}\".format(trial.number)), monitor=\"validation_loss\"\n",
    "    )\n",
    "\n",
    "    metrics_callback = MetricsCallback()\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=False,\n",
    "        checkpoint_callback=checkpoint_callback,\n",
    "        max_epochs=50,\n",
    "        gpus=1 if torch.cuda.is_available() else None,\n",
    "        callbacks=[metrics_callback, \n",
    "                   PyTorchLightningPruningCallback(trial, monitor=\"validation_loss\")],\n",
    "    )\n",
    "\n",
    "    model = AutoEncoder_HP(trial)\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    \n",
    "    return metrics_callback.metrics[-1][\"validation_loss\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(DIR, \"result\")\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=100, timeout=None)#600)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study, target_name='Validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study,  target_name='Validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the autoencoder with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "                                        # First convolutional layer\n",
    "                                        nn.Conv2d(1, 8, kernel_size=3, padding=1, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Second convolutional layer\n",
    "                                        nn.Conv2d(8, 16, kernel_size=3, padding=1, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Third convolutional layer\n",
    "                                        nn.Conv2d(16, 32, kernel_size=3, padding=0, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Flatten layer\n",
    "                                        nn.Flatten(start_dim=1),\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(288, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Second linear layer (output layer)\n",
    "                                        nn.Linear(64, encoded_space_dim)\n",
    "                                    )\n",
    "        # Decoder\n",
    "        self.decoder =  nn.Sequential(\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(encoded_space_dim, 64),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Second linear layer\n",
    "                                        nn.Linear(64, 3*3*32), # (64, 288)\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Unflatten\n",
    "                                        nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)),\n",
    "                                        # First transposed convolution\n",
    "                                        nn.ConvTranspose2d(32, 16, kernel_size=3, output_padding=0, stride=2),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Second transposed convolution\n",
    "                                        nn.ConvTranspose2d(16, 8, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Third transposed convolution\n",
    "                                        nn.ConvTranspose2d(8, 1, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                                        # To obtain an output in [0,1]\n",
    "                                        nn.Sigmoid()\n",
    "                                    )\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = self.loss_fn(x_hat, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, loss_name='validation_loss'):\n",
    "        x, y = batch\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        val_loss = self.loss_fn(x_hat, x)\n",
    "        self.log(loss_name, val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.validation_step(batch, batch_idx, loss_name='test_loss')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0008157670205790078, weight_decay=1.4786239067036069e-05)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_loss(self, loss_fn):\n",
    "        self.loss_fn = loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T16:13:50.145059Z",
     "start_time": "2020-11-26T16:13:50.118613Z"
    },
    "id": "lJvdqRsdTJxV"
   },
   "outputs": [],
   "source": [
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(49)\n",
    "np.random.seed(49)\n",
    "\n",
    "### Initialize the two networks\n",
    "encoded_space_dim = 9\n",
    "auto_enc = AutoEncoder(encoded_space_dim=encoded_space_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pd6vZXdbBAE"
   },
   "source": [
    "Let's check if all the shapes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T16:13:51.125024Z",
     "start_time": "2020-11-26T16:13:51.101812Z"
    },
    "id": "duzPT5MoLiK-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Some examples\n",
    "# Take an input image (remember to add the batch dimension)\n",
    "img, _ = test_dataset[0]\n",
    "img = img.unsqueeze(0) # Add the batch dimension in the first axis\n",
    "print('Original image shape:', img.shape)\n",
    "# Encode the image\n",
    "img_enc = auto_enc.forward(img)\n",
    "print('Encoded image shape:', img_enc.shape)\n",
    "# Decode the image\n",
    "dec_img = auto_enc.decoder(img_enc)\n",
    "print('Decoded image shape:', dec_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ04VzkibtCn"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T16:15:20.716208Z",
     "start_time": "2020-11-26T16:15:20.636834Z"
    },
    "id": "Ww_iF8xCLkfu"
   },
   "outputs": [],
   "source": [
    "### Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "auto_enc.to(device)\n",
    "auto_enc.configure_loss(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T16:27:40.460945Z",
     "start_time": "2020-11-26T16:27:40.377138Z"
    },
    "id": "E79E0W6hXB3q"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=100, progress_bar_refresh_rate=20, \n",
    "                     callbacks=[EarlyStopping(monitor='validation_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.fit(auto_enc, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViwqPa7XdW4_"
   },
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(auto_enc, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "trainer.save_checkpoint(\"best_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot some sample\n",
    "fig, axs = plt.subplots(2, 4, figsize=(8,6))\n",
    "for ax in axs:\n",
    "    # random.choice allows to randomly sample from a list-like object (basically anything that can be accessed with an index, like our dataset)\n",
    "    img, label = random.choice(test_dataset)\n",
    "    img1 = img[0]\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = auto_enc.encoder(img.unsqueeze(0).to(device))\n",
    "        decoded_img  = auto_enc.decoder(encoded_img)\n",
    "    \n",
    "    ax[0].imshow(np.array(img1), cmap='gist_gray')\n",
    "    ax[0].set_title('Original, Label: %d' % label)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "\n",
    "    ax[1].imshow(decoded_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "    ax[1].set_title('Reconstructed, Label: %d' % label)\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    \n",
    "    img, label = random.choice(test_dataset)\n",
    "    img1 = img[0]\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = auto_enc.encoder(img.unsqueeze(0).to(device))\n",
    "        decoded_img  = auto_enc.decoder(encoded_img)\n",
    "    \n",
    "    ax[2].imshow(np.array(img1), cmap='gist_gray')\n",
    "    ax[2].set_title('Original, Label: %d' % label)\n",
    "    ax[2].set_xticks([])\n",
    "    ax[2].set_yticks([])\n",
    "\n",
    "    ax[3].imshow(decoded_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "    ax[3].set_title('Reconstructed, Label: %d' % label)\n",
    "    ax[3].set_xticks([])\n",
    "    ax[3].set_yticks([])\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoiser\n",
    "To implement the denoiser we simply add some gaussian noise to the image during the training/evaluation procedures, using the not-noisy image for the computation of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "                                        # First convolutional layer\n",
    "                                        nn.Conv2d(1, 8, kernel_size=3, padding=1, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Second convolutional layer\n",
    "                                        nn.Conv2d(8, 16, kernel_size=3, padding=1, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Third convolutional layer\n",
    "                                        nn.Conv2d(16, 32, kernel_size=3, padding=0, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Flatten layer\n",
    "                                        nn.Flatten(start_dim=1),\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(288, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Second linear layer (output layer)\n",
    "                                        nn.Linear(64, encoded_space_dim)\n",
    "                                    )\n",
    "        # Decoder\n",
    "        self.decoder =  nn.Sequential(\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(encoded_space_dim, 64),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Second linear layer\n",
    "                                        nn.Linear(64, 3*3*32), # (64, 288)\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Unflatten\n",
    "                                        nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)),\n",
    "                                        # First transposed convolution\n",
    "                                        nn.ConvTranspose2d(32, 16, kernel_size=3, output_padding=0, stride=2),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Second transposed convolution\n",
    "                                        nn.ConvTranspose2d(16, 8, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Third transposed convolution\n",
    "                                        nn.ConvTranspose2d(8, 1, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                                        # To obtain an output in [0,1]\n",
    "                                        nn.Sigmoid()\n",
    "                                    )\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        # Apply noise to x\n",
    "        mean = torch.randn(1).to(device) * 1\n",
    "        std = torch.randn(1).to(device) * 0.5 + 0.5\n",
    "        noisy_x = x + torch.randn(x.size()).to(device) *std + mean\n",
    "        z = self.encoder(noisy_x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = self.loss_fn(x_hat, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, loss_name='validation_loss'):\n",
    "        x, y = batch\n",
    "        # Apply noise to x\n",
    "        mean = torch.randn(1).to(device) * 1\n",
    "        std = torch.randn(1).to(device) * 0.5 + 0.5\n",
    "        noisy_x = x + torch.randn(x.size()).to(device) *std + mean\n",
    "        z = self.encoder(noisy_x)\n",
    "        x_hat = self.decoder(z)\n",
    "        val_loss = self.loss_fn(x_hat, x)\n",
    "        self.log(loss_name, val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.validation_step(batch, batch_idx, loss_name='test_loss')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0008157670205790078, weight_decay=1.4786239067036069e-05)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_loss(self, loss_fn):\n",
    "        self.loss_fn = loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(49)\n",
    "np.random.seed(49)\n",
    "\n",
    "### Initialize the two networks\n",
    "encoded_space_dim = 9\n",
    "denoise = Denoiser(encoded_space_dim=encoded_space_dim)\n",
    "\n",
    "### Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "denoise.to(device)\n",
    "denoise.configure_loss(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=100, progress_bar_refresh_rate=20, \n",
    "                     callbacks=[EarlyStopping(monitor='validation_loss')])\n",
    "trainer.fit(denoise, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(denoise, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot some sample\n",
    "fig, axs = plt.subplots(3, 6, figsize=(10,6))\n",
    "for ax in axs:\n",
    "    # random.choice allows to randomly sample from a list-like object (basically anything that can be accessed with an index, like our dataset)\n",
    "    img, label = random.choice(test_dataset)\n",
    "    img1 = img[0]\n",
    "    ax[0].imshow(np.array(img1), cmap='gist_gray')\n",
    "    ax[0].set_title('Original, Label: %d' % label)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    \n",
    "    img1 = img1 + np.random.normal(0, 1, size=img1.shape)\n",
    "    ax[1].imshow(np.array(img1), cmap='gist_gray')\n",
    "    ax[1].set_title('Noisy, Label: %d' % label)\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    \n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    denoise.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = denoise.encoder(img)\n",
    "        decoded_img  = denoise.decoder(encoded_img)\n",
    "    ax[2].imshow(np.array(decoded_img.cpu()[0][0]), cmap='gist_gray')\n",
    "    ax[2].set_title('Decoded, Label: %d' % label)\n",
    "    ax[2].set_xticks([])\n",
    "    ax[2].set_yticks([])\n",
    "    \n",
    "    # random.choice allows to randomly sample from a list-like object (basically anything that can be accessed with an index, like our dataset)\n",
    "    img, label = random.choice(train_dataset)\n",
    "    img1 = img[0]\n",
    "    ax[3].imshow(np.array(img1), cmap='gist_gray')\n",
    "    ax[3].set_title('Original, Label: %d' % label)\n",
    "    ax[3].set_xticks([])\n",
    "    ax[3].set_yticks([])\n",
    "    \n",
    "    img1 = img1 + np.random.normal(0, 1, size=img1.shape)\n",
    "    ax[4].imshow(np.array(img1), cmap='gist_gray')\n",
    "    ax[4].set_title('Noisy, Label: %d' % label)\n",
    "    ax[4].set_xticks([])\n",
    "    ax[4].set_yticks([])\n",
    "    \n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    denoise.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = denoise.encoder(img)\n",
    "        decoded_img  = denoise.decoder(encoded_img)\n",
    "    ax[5].imshow(np.array(decoded_img.cpu()[0][0]), cmap='gist_gray')\n",
    "    ax[5].set_title('Decoded, Label: %d' % label)\n",
    "    ax[5].set_xticks([])\n",
    "    ax[5].set_yticks([])\n",
    "    \n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning\n",
    "We will apply the methods of transfer learning: we will keep all the network up to the encoded space fixed, and add on top of it a fully-connected layer and a readout layer. We will train only these last two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, encoded_space_dim, pretrained):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = pretrained.encoder\n",
    "        # Decoder\n",
    "        self.fine_tune =  nn.Sequential(\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(encoded_space_dim, 64),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Second linear layer\n",
    "                                        nn.Linear(64, 10),\n",
    "                                        nn.LogSoftmax()\n",
    "                                       \n",
    "                                    )\n",
    "        self.accuracy = pl.metrics.Accuracy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        x = self.encoder(x)\n",
    "        x = self.fine_tune(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        z = self.forward(x)\n",
    "        loss = self.loss_fn(z, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, loss_name='validation_loss'):\n",
    "        x, y = batch\n",
    "        z = self.forward(x)\n",
    "        val_loss = self.loss_fn(z, y)\n",
    "        self.log(loss_name, val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        z = self.forward(x)\n",
    "        self.log('accuracy', self.accuracy(z, y), prog_bar=True)\n",
    "        return self.accuracy(z, y)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.00013901410327079125, weight_decay=7.274106352015157e-05)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_loss(self, loss_fn):\n",
    "        self.loss_fn = loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained = AutoEncoder.load_from_checkpoint(checkpoint_path=\"best_model.ckpt\", encoded_space_dim=encoded_space_dim)\n",
    "fine_tuner = Classifier(encoded_space_dim, pretrained)\n",
    "\n",
    "# Freeze params\n",
    "for param in fine_tuner.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "neg_log_like = torch.nn.NLLLoss()\n",
    "\n",
    "fine_tuner.configure_loss(neg_log_like)\n",
    "fine_tuner.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=50, progress_bar_refresh_rate=20, \n",
    "                     callbacks=[EarlyStopping(monitor='validation_loss')])\n",
    "trainer.fit(fine_tuner, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "trainer.test(fine_tuner, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "We want to encode the samples into probability distribution. For a full explanation see the report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.g1 = nn.Sequential(\n",
    "                                        # First convolutional layer\n",
    "                                        nn.Conv2d(1, 8, kernel_size=3, padding=1, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Second convolutional layer\n",
    "                                        nn.Conv2d(8, 16, kernel_size=3, padding=1, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Third convolutional layer\n",
    "                                        nn.Conv2d(16, 32, kernel_size=3, padding=0, stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Flatten layer\n",
    "                                        nn.Flatten(start_dim=1)\n",
    "                                )\n",
    "        self.g2 = nn.Sequential(\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(288, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Second linear layer (output layer). Average of gaussian distr\n",
    "                                        nn.Linear(64, encoded_space_dim)\n",
    "                                        \n",
    "                                    )\n",
    "        \n",
    "        self.h2 = nn.Sequential(\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(288, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        # Second linear layer (output layer). Diagonal of covariance matrix\n",
    "                                        nn.Linear(64, encoded_space_dim)\n",
    "                                    )\n",
    "        # Decoder\n",
    "        self.decoder =  nn.Sequential(\n",
    "                                        # First linear layer\n",
    "                                        nn.Linear(encoded_space_dim, 64),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Second linear layer\n",
    "                                        nn.Linear(64, 3*3*32), # (64, 288)\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Unflatten\n",
    "                                        nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)),\n",
    "                                        # First transposed convolution\n",
    "                                        nn.ConvTranspose2d(32, 16, kernel_size=3, output_padding=0, stride=2),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Second transposed convolution\n",
    "                                        nn.ConvTranspose2d(16, 8, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                                        nn.ReLU(True),\n",
    "                                        # Third transposed convolution\n",
    "                                        nn.ConvTranspose2d(8, 1, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                                        # To obtain an output in [0,1]\n",
    "                                        nn.Sigmoid()\n",
    "                                    )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        encode = self.g1(x)\n",
    "        means = self.g2(encode)\n",
    "        cov = self.h2(encode)\n",
    "        sample = cov*torch.randn(cov.size()).to(self.device) + means\n",
    "        return sample\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        encode = self.g1(x)\n",
    "        means = self.g2(encode)\n",
    "        cov = self.h2(encode)\n",
    "        # Sampling\n",
    "        z = cov*torch.randn(cov.size()).to(self.device) + means\n",
    "        \n",
    "        x_hat = self.decoder(z)\n",
    "        loss = self.loss_fn(x_hat, x) + self.kl_reg*self._kl_div(means, cov)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, loss_name='validation_loss'):\n",
    "        x, y = batch\n",
    "        encode = self.g1(x)\n",
    "        means = self.g2(encode)\n",
    "        cov = self.h2(encode)\n",
    "        z = cov*torch.randn(cov.size()).to(self.device) + means\n",
    "        \n",
    "        x_hat = self.decoder(z)\n",
    "        val_loss = self.loss_fn(x_hat, x) + self.kl_reg*self._kl_div(means, cov)\n",
    "        self.log(loss_name, val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.validation_step(batch, batch_idx, loss_name='test_loss')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0008157670205790078, weight_decay=1.4786239067036069e-05)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_loss(self, loss_fn, kl_reg):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.kl_reg = kl_reg\n",
    "        \n",
    "    def _kl_div(self, means, stds):\n",
    "        kl = 0.5*torch.sum( stds**2+means**2-1-torch.log( stds**2 ) )\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(49)\n",
    "np.random.seed(49)\n",
    "\n",
    "### Initialize the two networks\n",
    "encoded_space_dim = 9\n",
    "vae = VAE(encoded_space_dim=encoded_space_dim)\n",
    "\n",
    "### Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "vae.to(device)\n",
    "vae.configure_loss(loss_fn, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=100, progress_bar_refresh_rate=20, \n",
    "                     callbacks=[EarlyStopping(monitor='validation_loss')])\n",
    "trainer.fit(vae, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "trainer.test(vae, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"best_vae.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T16:53:13.852713Z",
     "start_time": "2020-11-26T16:53:13.484547Z"
    },
    "id": "uHxpafEzg01Q",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img, label = random.choice(test_dataset)\n",
    "original = img.squeeze().numpy()\n",
    "img = img.unsqueeze(0).to(device)\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    encoded_samples_start  = vae.forward(img)\n",
    "\n",
    "if encoded_space_dim == 9:\n",
    "    # Decode sample\n",
    "    auto_enc.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_img  = vae.decoder(encoded_samples_start+1.5*torch.randn(encoded_samples_start.size()).to(device))\n",
    "        decoded_img  = vae.decoder(encoded_samples_start)\n",
    "        \n",
    "    fig, ax = plt.subplots(2, 6, figsize=(16,4))\n",
    "    \n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    ax[0].set_title(f'Original, Label: {label}')\n",
    "    ax[0].imshow(original, cmap='gist_gray')\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    \n",
    "    ax[1].set_title(f'Decoded')\n",
    "    ax[1].imshow(decoded_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    \n",
    "    for i in range(2, len(ax)):\n",
    "        k = 0\n",
    "        shift = torch.zeros( encoded_samples_start.size() ).to(device)\n",
    "        shift[0][k] += encoded_samples_start[0][k]/5*i\n",
    "        with torch.no_grad():\n",
    "            generated_img  = vae.decoder(encoded_samples_start+shift)\n",
    "        ax[i].set_title('Shift: %.3f' %(shift[0][k]) )\n",
    "        ax[i].imshow(generated_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoded space analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical Autencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T16:39:50.388484Z",
     "start_time": "2020-11-26T16:39:50.281784Z"
    },
    "id": "TRbQ5LtkMbr-"
   },
   "outputs": [],
   "source": [
    "# Load network parameters\n",
    "auto_enc = AutoEncoder.load_from_checkpoint(\"best_model.ckpt\", encoded_space_dim=encoded_space_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T16:40:15.050432Z",
     "start_time": "2020-11-26T16:39:56.658519Z"
    },
    "id": "bi2ZD1ALkVaK"
   },
   "outputs": [],
   "source": [
    "### Get the encoded representation of the test samples\n",
    "encoded_samples = []\n",
    "enc_aut = np.zeros(encoded_space_dim+1)\n",
    "for sample in tqdm(test_dataset):\n",
    "    img = sample[0].unsqueeze(0).to(device)\n",
    "    label = sample[1]\n",
    "    # Encode image\n",
    "    auto_enc.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = auto_enc.encoder(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    encoded_sample['label'] = label\n",
    "    encoded_samples.append(encoded_sample)\n",
    "    enc_aut = np.vstack( (enc_aut, np.hstack( (encoded_img, label)) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T16:40:24.013762Z",
     "start_time": "2020-11-26T16:40:23.618998Z"
    },
    "id": "27_GEZetobW7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "\n",
    "# PCA\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(encoded_samples.iloc[:,0:encoded_space_dim])\n",
    "columns = [ f'Enc. Variable {i}' for i in range(n_components)]\n",
    "compressed_samples = pd.DataFrame( pca.transform(encoded_samples.iloc[:,0:encoded_space_dim]), columns=columns )\n",
    "\n",
    "if n_components == 2:\n",
    "    fig = px.scatter(compressed_samples, x='Enc. Variable 0', y='Enc. Variable 1', \n",
    "           color=encoded_samples.label.astype(str), opacity=0.7)\n",
    "elif n_components == 3:\n",
    "    fig = px.scatter_3d(compressed_samples,  x='Enc. Variable 0', y='Enc. Variable 1', z='Enc. Variable 2',\n",
    "                    color=encoded_samples.label.astype(str), opacity=0.7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "\n",
    "# PCA\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components=n_components)\n",
    "tsne.fit(encoded_samples.iloc[:, 0:encoded_space_dim])\n",
    "columns = [ f'Enc. Variable {i}' for i in range(n_components)]\n",
    "compressed_samples = pd.DataFrame( tsne.fit_transform(encoded_samples.iloc[:,0:encoded_space_dim]), columns=columns )\n",
    "\n",
    "if n_components == 2:\n",
    "    fig = px.scatter(compressed_samples, x='Enc. Variable 0', y='Enc. Variable 1', \n",
    "           color=encoded_samples.label.astype(str), opacity=0.7)\n",
    "elif n_components == 3:\n",
    "    fig = px.scatter_3d(compressed_samples,  x='Enc. Variable 0', y='Enc. Variable 1', z='Enc. Variable 2',\n",
    "                    color=encoded_samples.label.astype(str), opacity=0.7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compr = pca.transform(enc_aut[:,:encoded_space_dim]) \n",
    "compr = np.hstack( (compr, enc_aut[:, -1].reshape(10001, 1) ) )\n",
    "pca_aut = np.zeros(4)\n",
    "for i in range(10):\n",
    "    means = compr[ compr[:, -1]==i ].mean(axis=0)\n",
    "    stds = compr[ compr[:, -1]==i ].std(axis=0)\n",
    "    tot = np.hstack( (means[:2], stds[:2]) )\n",
    "    pca_aut = np.vstack( (pca_aut, tot) )\n",
    "    \n",
    "pca_aut = pca_aut[1:, :]\n",
    "\n",
    "compr = tsne.fit_transform(enc_aut[:,:encoded_space_dim]) \n",
    "compr = np.hstack( (compr, enc_aut[:, -1].reshape(10001, 1) ) )\n",
    "tsne_aut = np.zeros(4)\n",
    "for i in range(10):\n",
    "    means = compr[ compr[:, -1]==i ].mean(axis=0)\n",
    "    stds = compr[ compr[:, -1]==i ].std(axis=0)\n",
    "    tot = np.hstack( (means[:2], stds[:2]) )\n",
    "    tsne_aut = np.vstack( (tsne_aut, tot) )\n",
    "    \n",
    "tsne_aut = tsne_aut[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average difference of the two standard deviation along the two dimension.\n",
    "# If the distribution is perfectly gaussian then the difference should be 0\n",
    "avg_diff = [ np.abs(pca_aut[:, 2]- pca_aut[:, 3]).mean() , np.abs(tsne_aut[:, 2]- tsne_aut[:, 3]).mean() ]\n",
    "print(avg_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = ['pink', 'cyan', 'red', 'lime', 'green', 'orange', 'gray', 'black', 'navy', 'purple']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ellipse = Ellipse((pca_aut[i, 0], pca_aut[i, 1]), width=pca_aut[i, 2] * 2, height=pca_aut[i, 3] * 2, \n",
    "                      alpha=0.5, facecolor = colors[i], label=str(i))\n",
    "    \n",
    "    ax[0].add_patch(ellipse)\n",
    "ax[0].legend()\n",
    "    \n",
    "ax[0].scatter(pca_aut[:, 0], pca_aut[:, 1], color='black')\n",
    "ax[0].set_xlim( (1.5*pca_aut[:, 0].min(), 1.5*pca_aut[:, 0].max()) )\n",
    "ax[0].set_ylim( (1.5*pca_aut[:, 1].min(), 1.5*pca_aut[:, 1].max()) )\n",
    "ax[0].set_xlabel('Encoded variable 0')\n",
    "ax[0].set_ylabel('Encoded variable 1')\n",
    "ax[0].set_title('PCA', fontsize=16)\n",
    "\n",
    "for i in range(10):\n",
    "    ellipse = Ellipse((tsne_aut[i, 0], tsne_aut[i, 1]), width=tsne_aut[i, 2] * 2, height=tsne_aut[i, 3] * 2, \n",
    "                      alpha=0.5, facecolor = colors[i], label=str(i))\n",
    "    \n",
    "    ax[1].add_patch(ellipse)\n",
    "ax[1].legend()\n",
    "    \n",
    "ax[1].scatter(tsne_aut[:, 0], tsne_aut[:, 1], color='black')\n",
    "ax[1].set_xlim( (1.5*tsne_aut[:, 0].min(), 1.5*tsne_aut[:, 0].max()) )\n",
    "ax[1].set_ylim( (1.5*tsne_aut[:, 1].min(), 1.5*tsne_aut[:, 1].max()) )\n",
    "ax[1].set_xlabel('Encoded variable 0')\n",
    "ax[1].set_ylabel('Encoded variable 1')\n",
    "ax[1].set_title('t-SNE', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of new samples\n",
    "### Plot some sample\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8,6))\n",
    "# random.choice allows to randomly sample from a list-like object (basically anything that can be accessed with an index, like our dataset)\n",
    "img, label = random.choice(test_dataset)\n",
    "img1 = img[0]\n",
    "with torch.no_grad():\n",
    "    encoded_img  = auto_enc.encoder(img.unsqueeze(0).to(device))\n",
    "    decoded_img  = auto_enc.decoder(20*torch.randn_like(encoded_img).to(device) )\n",
    "\n",
    "ax[0].imshow( decoded_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "ax[0].set_title('Generated sample' )\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_img  = auto_enc.decoder(20*torch.randn_like(encoded_img).to(device) )\n",
    "\n",
    "ax[1].imshow(decoded_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "ax[1].set_title('Generated sample' )\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "img, label = random.choice(test_dataset)\n",
    "img1 = img[0]\n",
    "with torch.no_grad():\n",
    "    decoded_img  = auto_enc.decoder(20*torch.randn_like(encoded_img).to(device) )\n",
    "\n",
    "ax[2].imshow(decoded_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "ax[2].set_title('Generated sample' )\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network parameters\n",
    "vae = VAE.load_from_checkpoint(\"best_vae.ckpt\", encoded_space_dim=encoded_space_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vae = np.zeros(encoded_space_dim+1)\n",
    "### Get the encoded representation of the test samples\n",
    "encoded_samples = []\n",
    "vae.to(device)\n",
    "for sample in tqdm(test_dataset):\n",
    "    img = sample[0].unsqueeze(0).to(device)\n",
    "    label = sample[1]\n",
    "    # Encode image\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = vae.forward(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    encoded_sample['label'] = label\n",
    "    encoded_samples.append(encoded_sample)\n",
    "    enc_vae = np.vstack( (enc_vae, np.hstack((encoded_img, label))  ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(encoded_samples.iloc[:, 0:encoded_space_dim])\n",
    "columns = [ f'Enc. Variable {i}' for i in range(n_components)]\n",
    "compressed_samples = pd.DataFrame( pca.transform(encoded_samples.iloc[:, 0:encoded_space_dim]), columns=columns )\n",
    "\n",
    "if n_components == 2:\n",
    "    fig = px.scatter(compressed_samples, x='Enc. Variable 0', y='Enc. Variable 1', \n",
    "           color=encoded_samples.label.astype(str), opacity=0.7)\n",
    "elif n_components == 3:\n",
    "    fig = px.scatter_3d(compressed_samples,  x='Enc. Variable 0', y='Enc. Variable 1', z='Enc. Variable 2',\n",
    "                    color=encoded_samples.label.astype(str), opacity=0.7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "\n",
    "# PCA\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components=n_components)\n",
    "tsne.fit(encoded_samples.iloc[:, 0:encoded_space_dim])\n",
    "columns = [ f'Enc. Variable {i}' for i in range(n_components)]\n",
    "compressed_samples = pd.DataFrame( tsne.fit_transform(encoded_samples.iloc[:,0:encoded_space_dim]), columns=columns )\n",
    "\n",
    "if n_components == 2:\n",
    "    fig = px.scatter(compressed_samples, x='Enc. Variable 0', y='Enc. Variable 1', \n",
    "           color=encoded_samples.label.astype(str), opacity=0.7)\n",
    "elif n_components == 3:\n",
    "    fig = px.scatter_3d(compressed_samples,  x='Enc. Variable 0', y='Enc. Variable 1', z='Enc. Variable 2',\n",
    "                    color=encoded_samples.label.astype(str), opacity=0.7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compr = pca.transform(enc_vae[:,:encoded_space_dim]) \n",
    "compr = np.hstack( (compr, enc_vae[:, -1].reshape(10001, 1) ) )\n",
    "pca_vae = np.zeros(4)\n",
    "for i in range(10):\n",
    "    means = compr[ compr[:, -1]==i ].mean(axis=0)\n",
    "    stds = compr[ compr[:, -1]==i ].std(axis=0)\n",
    "    tot = np.hstack( (means[:2], stds[:2]) )\n",
    "    pca_vae = np.vstack( (pca_vae, tot) )\n",
    "    \n",
    "pca_vae = pca_vae[1:, :]\n",
    "\n",
    "compr = tsne.fit_transform(enc_vae[:,:encoded_space_dim]) \n",
    "compr = np.hstack( (compr, enc_vae[:, -1].reshape(10001, 1) ) )\n",
    "tsne_vae = np.zeros(4)\n",
    "for i in range(10):\n",
    "    means = compr[ compr[:, -1]==i ].mean(axis=0)\n",
    "    stds = compr[ compr[:, -1]==i ].std(axis=0)\n",
    "    tot = np.hstack( (means[:2], stds[:2]) )\n",
    "    tsne_vae = np.vstack( (tsne_vae, tot) )\n",
    "    \n",
    "tsne_vae = tsne_vae[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average difference of the two standard deviation along the two dimension.\n",
    "# If the distribution is perfectly gaussian then the difference should be 0\n",
    "avg_diff = [ np.abs(pca_vae[:, 2]- pca_vae[:, 3]).mean() , np.abs(tsne_vae[:, 2]- tsne_vae[:, 3]).mean() ]\n",
    "print(avg_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['pink', 'cyan', 'red', 'lime', 'green', 'orange', 'gray', 'black', 'navy', 'purple']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ellipse = Ellipse((pca_vae[i, 0], pca_vae[i, 1]), width=pca_vae[i, 2] * 2, height=pca_vae[i, 3] * 2, \n",
    "                      alpha=0.5, facecolor = colors[i], label=str(i))\n",
    "    \n",
    "    ax[0].add_patch(ellipse)\n",
    "ax[0].legend()\n",
    "    \n",
    "ax[0].scatter(pca_vae[:, 0], pca_vae[:, 1], color='black')\n",
    "ax[0].set_xlim( (1.5*pca_vae[:, 0].min(), 1.5*pca_vae[:, 0].max()) )\n",
    "ax[0].set_ylim( (1.5*pca_vae[:, 1].min(), 1.5*pca_vae[:, 1].max()) )\n",
    "ax[0].set_xlabel('Encoded variable 0')\n",
    "ax[0].set_ylabel('Encoded variable 1')\n",
    "ax[0].set_title('PCA', fontsize=16)\n",
    "\n",
    "for i in range(10):\n",
    "    ellipse = Ellipse((tsne_vae[i, 0], tsne_vae[i, 1]), width=tsne_vae[i, 2] * 2, height=tsne_vae[i, 3] * 2, \n",
    "                      alpha=0.5, facecolor = colors[i], label=str(i))\n",
    "    \n",
    "    ax[1].add_patch(ellipse)\n",
    "ax[1].legend()\n",
    "    \n",
    "ax[1].scatter(tsne_vae[:, 0], tsne_vae[:, 1], color='black')\n",
    "ax[1].set_xlim( (1.5*tsne_vae[:, 0].min(), 1.5*tsne_vae[:, 0].max()) )\n",
    "ax[1].set_ylim( (1.5*tsne_vae[:, 1].min(), 1.5*tsne_vae[:, 1].max()) )\n",
    "ax[1].set_xlabel('Encoded variable 0')\n",
    "ax[1].set_ylabel('Encoded variable 1')\n",
    "ax[1].set_title('t-SNE', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of new samples\n",
    "### Plot some sample\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8,6))\n",
    "# random.choice allows to randomly sample from a list-like object (basically anything that can be accessed with an index, like our dataset)\n",
    "img, label = random.choice(test_dataset)\n",
    "img1 = img[0]\n",
    "with torch.no_grad():\n",
    "    encoded_img  = vae.forward(img.unsqueeze(0).to(device))\n",
    "    decoded_img  = vae.decoder(torch.randn_like(encoded_img).to(device) )\n",
    "\n",
    "ax[0].imshow( decoded_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "ax[0].set_title('Generated sample' )\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_img  = vae.decoder(torch.randn_like(encoded_img).to(device) )\n",
    "\n",
    "ax[1].imshow(decoded_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "ax[1].set_title('Generated sample' )\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "img, label = random.choice(test_dataset)\n",
    "img1 = img[0]\n",
    "with torch.no_grad():\n",
    "    decoded_img  = vae.decoder(torch.randn_like(encoded_img).to(device) )\n",
    "\n",
    "ax[2].imshow(decoded_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "ax[2].set_title('Generated sample' )\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMHQyxe3n9M+hEJCxOrDMnr",
   "collapsed_sections": [],
   "name": "nndl_2020_lab_05_convolutional_autoencoder.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1w5PwADM98HzeKtV6-LZitxLkktJBx2V5",
     "timestamp": 1602430537656
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
