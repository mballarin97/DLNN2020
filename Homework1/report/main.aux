\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bbl@cs{beforestart}
\abx@aux@refcontext{none/global//global/global}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\newlabel{sec:int}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction }{1}{section.1}\protected@file@percent }
\newlabel{sec:reg}{{2}{1}{Regression}{section.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Regression }{1}{section.2}\protected@file@percent }
\newlabel{sec:meth_reg}{{2.1}{1}{Methods}{subsection.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Methods }{1}{subsection.2.1}\protected@file@percent }
\newlabel{sec:res_reg}{{2.2}{2}{Results}{subsection.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Results }{2}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Final network for the regression task. The activation function is the rectified linear unit.\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:reg_net}{{1}{2}{Final network for the regression task. The activation function is the rectified linear unit.\relax }{figure.caption.1}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Optimal parameters for the regression task.\relax }}{2}{table.caption.2}\protected@file@percent }
\newlabel{tab:reg_par}{{1}{2}{Optimal parameters for the regression task.\relax }{table.caption.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Data points with the model prediction. We can see that the model is able to reproduce the first maximum, but cuts the second.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{tab:reg_pred}{{2}{3}{Data points with the model prediction. We can see that the model is able to reproduce the first maximum, but cuts the second.\relax }{figure.caption.3}{}}
\newlabel{sec:class}{{3}{3}{Classification}{section.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Classification }{3}{section.3}\protected@file@percent }
\newlabel{sec:meth_class}{{3.1}{3}{Methods}{subsection.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Methods }{3}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Activation profile of the first hidden layer. In thick red we can see the average of the profiles of the neurons, multiplied by a constant to better catch the behavior. It is clear that this layer is able to understand if the input is on the left or right side of the input domain.\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:reg_h1}{{3}{4}{Activation profile of the first hidden layer. In thick red we can see the average of the profiles of the neurons, multiplied by a constant to better catch the behavior. It is clear that this layer is able to understand if the input is on the left or right side of the input domain.\relax }{figure.caption.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Activation profile of the second hidden layer. The average enhanced activation is defined as in Figure \ref  {fig:reg_h1}. We can notice from this plot that the first maximum is well represented, while the second one is cut even if some of the neurons actually are particularly active in that interval.\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:reg_h2}{{4}{4}{Activation profile of the second hidden layer. The average enhanced activation is defined as in Figure \ref {fig:reg_h1}. We can notice from this plot that the first maximum is well represented, while the second one is cut even if some of the neurons actually are particularly active in that interval.\relax }{figure.caption.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Network structure for the classification task. The activation function used is the rectified linear unit. The output digit is selected as the argmax among the $10$ output neurons.\relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{fig:cl_net}{{5}{4}{Network structure for the classification task. The activation function used is the rectified linear unit. The output digit is selected as the argmax among the $10$ output neurons.\relax }{figure.caption.5}{}}
\newlabel{sec:res_class}{{3.2}{5}{Results}{subsection.3.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Results }{5}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Optimal parameters for the classification task.\relax }}{5}{table.caption.6}\protected@file@percent }
\newlabel{tab:cl_par}{{2}{5}{Optimal parameters for the classification task.\relax }{table.caption.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces $3\times 3$ kernels of the filters of the first convolutional layer. It is not particularly clear which are the enhanced features. \relax }}{5}{figure.caption.7}\protected@file@percent }
\newlabel{fig:cl_f1}{{6}{5}{$3\times 3$ kernels of the filters of the first convolutional layer. It is not particularly clear which are the enhanced features. \relax }{figure.caption.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces $5\times 5$ kernels of the filters of the second convolutional layer. We can observe black lines, which correspond to an enhancement of the edges.\relax }}{5}{figure.caption.7}\protected@file@percent }
\newlabel{fig:cl_f2}{{7}{5}{$5\times 5$ kernels of the filters of the second convolutional layer. We can observe black lines, which correspond to an enhancement of the edges.\relax }{figure.caption.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The three channels in output from the first convolutional layer. We can observe the input digit, which is a $7$. \relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:cl_a1}{{8}{5}{The three channels in output from the first convolutional layer. We can observe the input digit, which is a $7$. \relax }{figure.caption.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The six channels in output from the second convolutional layer. We can observe a decomposition of the input digit in edges, which is common in convolutional networks. \relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:cl_a2}{{9}{5}{The six channels in output from the second convolutional layer. We can observe a decomposition of the input digit in edges, which is common in convolutional networks. \relax }{figure.caption.8}{}}
\newlabel{sec:app}{{4}{6}{Appendix}{section.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Appendix }{6}{section.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Weight histograms for the different layers of the network.\relax }}{6}{figure.caption.9}\protected@file@percent }
\newlabel{fig:reg_weights}{{10}{6}{Weight histograms for the different layers of the network.\relax }{figure.caption.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Hyperparameters search for the classification task. We plotted with a color which is proportional to the loss function, reported in log-scale. The darker the color, the better the result. We notice that a really high L2 regularization usually translates in a low loss score.\relax }}{6}{figure.caption.10}\protected@file@percent }
\newlabel{fig:cl_hp}{{11}{6}{Hyperparameters search for the classification task. We plotted with a color which is proportional to the loss function, reported in log-scale. The darker the color, the better the result. We notice that a really high L2 regularization usually translates in a low loss score.\relax }{figure.caption.10}{}}
